{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entity_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKBP08Xfq4gmi3sbzmiQ7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rllima/IF704/blob/main/entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHo8FQhOIYlx",
        "outputId": "829b4bd5-170e-4cba-c71c-a7ff37d3b061"
      },
      "source": [
        "#http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdgpa8T5V_Do",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c85c8c-fca5-4f8c-e722-f2ecf27a753a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rllima/IF704/main/data/train.csv\n",
        "!wget https://raw.githubusercontent.com/rllima/IF704/main/data/test.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-18 20:45:22--  https://raw.githubusercontent.com/rllima/IF704/main/data/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 354161 (346K) [text/plain]\n",
            "Saving to: ‘train.csv.2’\n",
            "\n",
            "\rtrain.csv.2           0%[                    ]       0  --.-KB/s               \rtrain.csv.2         100%[===================>] 345.86K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-08-18 20:45:22 (95.7 MB/s) - ‘train.csv.2’ saved [354161/354161]\n",
            "\n",
            "--2021-08-18 20:45:22--  https://raw.githubusercontent.com/rllima/IF704/main/data/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 568765 (555K) [text/plain]\n",
            "Saving to: ‘test.csv.2’\n",
            "\n",
            "test.csv.2          100%[===================>] 555.43K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-08-18 20:45:22 (125 MB/s) - ‘test.csv.2’ saved [568765/568765]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ARiDXiWv3X"
      },
      "source": [
        "from collections import defaultdict\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njvOebSiqlP2"
      },
      "source": [
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            # Replace all urls with <URL> token\n",
        "            # Replace all users with <USR> token\n",
        "            if token.find('http://') == 0 or token.find('https://') == 0:\n",
        "                token = '<URL>'\n",
        "            if token[0] == '@':\n",
        "                token = '<USR>'\n",
        "            \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO--dNtIwfjo"
      },
      "source": [
        "Prepare dictionaries\n",
        "\n",
        "To train a neural network, we will use two mappings:\n",
        "\n",
        "{token} →\n",
        "→\n",
        " {token id}: address the row in embeddings matrix for the current token;\n",
        "{tag} →\n",
        "→\n",
        " {tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
        "Now you need to implement the function build_dict which will return {token or tag} →\n",
        "→\n",
        " {index} and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OetWT43dvP0V"
      },
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
        "    # Add special tokens (or tags) to the dictionaries.\n",
        "    # The first special token must have index 0.\n",
        "    \n",
        "    # Mapping tok2idx should contain each token or tag only once. \n",
        "    # To do so, you should extract unique tokens/tags from the tokens_or_tags variable\n",
        "    # and then index them (for example, you can add them into the list idx2tok\n",
        "    # and for each token/tag save the index into tok2idx).\n",
        "\n",
        "    for twt in tokens_or_tags:\n",
        "        for tok in twt:\n",
        "            idx2tok.append(tok)\n",
        "    idx2tok = list(set(idx2tok))\n",
        "    idx2tok = special_tokens + idx2tok\n",
        "    for i, v in enumerate(idx2tok):\n",
        "        tok2idx[v] = i\n",
        "    \n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUw3A1jff7Xj"
      },
      "source": [
        "train_tokens, train_tags = read_data('train.csv')\n",
        "test_tokens, test_tags = read_data('test.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NTaq2SEWzJm"
      },
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTCcHeTbgi5S",
        "outputId": "a2348282-bce1-4395-a888-17a5bf33efed"
      },
      "source": [
        "len(set(idx2token)) == len(idx2token)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX4OVB8Li2Is"
      },
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8RS6JjgxA-f"
      },
      "source": [
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaT3vNOC1bUT"
      },
      "source": [
        "class BiLSTMModel():\n",
        "    pass"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpeyQWIX0WkZ"
      },
      "source": [
        "def declare_placeholders(self):\n",
        "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
        "\n",
        "    # Placeholders for input and ground truth output.\n",
        "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
        "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags') \n",
        "  \n",
        "    # Placeholder for lengths of the sequences.\n",
        "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        "    \n",
        "    # Placeholder for a dropout keep probability. If we don't feed\n",
        "    # a value for this placeholder, it will be equal to 1.0.\n",
        "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        "    \n",
        "    # Placeholder for a learning rate (tf.float32).\n",
        "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMBo9uTN1QJ-"
      },
      "source": [
        "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1egcJX51r8T"
      },
      "source": [
        "Now, let us specify the layers of the neural network. \n",
        "First, we need to perform some preparatory steps:\n",
        "\n",
        "Create embeddings matrix with tf.Variable. \n",
        "Specify its name (embeddings_matrix), type (tf.float32), and initialize with random values.\n",
        "Create forward and backward LSTM cells. TensorFlow provides a number of RNN cells ready for you.\n",
        "Wrap your cells with DropoutWrapper. Dropout is an important regularization technique for neural networks. Specify all keep probabilities using the dropout placeholder that we created before.\n",
        "After that, you can build the computation graph that transforms an input_batch:\n",
        "\n",
        "Look up embeddings for an input_batch in the prepared embedding_matrix.\n",
        "Pass the embeddings through Bidirectional Dynamic RNN with the specified forward and backward cells. Use the lengths placeholder here to avoid computations for padding tokens inside the RNN.\n",
        "Create a dense layer on top. Its output will be used directly in loss function.\n",
        "Fill in the code below. In case you need to debug something, the easiest way is to check that tensor shapes of each step match the expected ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yphF7PYE1sbt"
      },
      "source": [
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
        "    \n",
        "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
        "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "    embedding_matrix_variable = tf.Variable(initial_embedding_matrix, dtype=tf.float32)\n",
        "    ######### YOUR CODE HERE #############\n",
        "    \n",
        "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
        "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
        "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(n_hidden_rnn),\n",
        "                                                input_keep_prob=self.dropout_ph,\n",
        "                                                output_keep_prob=self.dropout_ph,\n",
        "                                                state_keep_prob=self.dropout_ph)\n",
        "    ######### YOUR CODE HERE #############\n",
        "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(n_hidden_rnn),\n",
        "                                                input_keep_prob=self.dropout_ph,\n",
        "                                                output_keep_prob=self.dropout_ph,\n",
        "                                                state_keep_prob=self.dropout_ph)\n",
        "    ######### YOUR CODE HERE #############\n",
        "\n",
        "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
        "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
        "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
        "    ######### YOUR CODE HERE #############\n",
        "    \n",
        "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
        "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
        "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
        "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(\n",
        "    forward_cell,\n",
        "    backward_cell,\n",
        "    embeddings,\n",
        "    dtype=tf.float32,\n",
        "    sequence_length=self.lengths)\n",
        "    ######### YOUR CODE HERE #############\n",
        "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "\n",
        "    # Dense layer on top.\n",
        "    # Shape: [batch_size, sequence_len, n_tags].   \n",
        "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnOkxxTs2Gvt"
      },
      "source": [
        "BiLSTMModel.__build_layers = classmethod(build_layers)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaG-z-YK2Ojg"
      },
      "source": [
        "To compute the actual predictions of the neural network, you need to apply softmax to the last layer and find the most probable tags with argmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUs3YHJy2P9C"
      },
      "source": [
        "def compute_predictions(self):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
        "    \n",
        "    # Create softmax (tf.nn.softmax) function\n",
        "    softmax_output = tf.nn.softmax(self.logits)\n",
        "    \n",
        "    # Use argmax (tf.argmax) to get the most probable tags\n",
        "    # Don't forget to set axis=-1\n",
        "    # otherwise argmax will be calculated in a wrong way\n",
        "    self.predictions = tf.argmax(softmax_output, axis = -1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93dZhUPf2n4q"
      },
      "source": [
        "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5TWaXzO2w3n"
      },
      "source": [
        "During training we do not need predictions of the network, but we need a loss function. We will use cross-entropy loss, efficiently implemented in TF as cross entropy with logits. Note that it should be applied to logits of the model (not to softmax probabilities!). Also note, that we do not want to take into account loss terms coming from <PAD> tokens. So we need to mask them out, before computing mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnE2kjfl2xSB"
      },
      "source": [
        "def compute_loss(self, n_tags, PAD_index):\n",
        "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
        "    \n",
        "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
        "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_tags_one_hot,\n",
        "                                                           logits=self.logits)\n",
        "    \n",
        "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
        "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
        "    mask = tf.cast(tf.not_equal(loss_tensor, PAD_index), tf.float32)\n",
        "    self.loss =  tf.reduce_mean(mask*loss_tensor)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wNFwJlI21Fj"
      },
      "source": [
        "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG8lSRRS3EWg"
      },
      "source": [
        "The last thing to specify is how we want to optimize the loss. We suggest that you use Adam optimizer with a learning rate from the corresponding placeholder. You will also need to apply clipping to eliminate exploding gradients. It can be easily done with clip_by_norm function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7QAIQiH3Erw"
      },
      "source": [
        "def perform_optimization(self):\n",
        "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
        "    \n",
        "    # Create an optimizer (tf.train.AdamOptimizer)\n",
        "    self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
        "    ######### YOUR CODE HERE #############\n",
        "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "    \n",
        "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
        "    # Pay attention that you need to apply this operation only for gradients \n",
        "    # because self.grads_and_vars contains also variables.\n",
        "    # list comprehension might be useful in this case.\n",
        "    clip_norm = tf.cast(1.0, tf.float32)\n",
        "    self.grads_and_vars = [(tf.clip_by_norm(g, clip_norm), v) for g,v in self.grads_and_vars]\n",
        "    \n",
        "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-mdLvDm3Igr"
      },
      "source": [
        "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOc57J2B3NSJ"
      },
      "source": [
        "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(n_tags, PAD_index)\n",
        "    self.__perform_optimization()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHQzWP8h3RMv"
      },
      "source": [
        "BiLSTMModel.__init__ = classmethod(init_model)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8U7yf-I34Lo"
      },
      "source": [
        "Train the network and predict tags\n",
        "Session.run is a point which initiates computations in the graph that we have defined. To train the network, we need to compute self.train_op, which was declared in perform_optimization. To predict tags, we just need to compute self.predictions. Anyway, we need to feed actual data through the placeholders that we defined before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m_6K4Ne34tt"
      },
      "source": [
        "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.ground_truth_tags: y_batch,\n",
        "                 self.learning_rate_ph: learning_rate,\n",
        "                 self.dropout_ph: dropout_keep_probability,\n",
        "                 self.lengths: lengths}\n",
        "    \n",
        "    session.run(self.train_op, feed_dict=feed_dict)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlFFQDsu4AIP"
      },
      "source": [
        "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C39g9nTl4CgX"
      },
      "source": [
        "def predict_for_batch(self, session, x_batch, lengths):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                self.lengths: lengths}\n",
        "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
        "    return predictions"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdgtI3DMALzb"
      },
      "source": [
        "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trO_b37L4F34"
      },
      "source": [
        "def predict_tags(model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxaCZO0-8EYs"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def _update_chunk(candidate, prev, current_tag, current_chunk, current_pos, prediction=False):\n",
        "    if candidate == 'B-' + current_tag:\n",
        "        if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
        "                current_chunk[-1].append(current_pos - 1)\n",
        "        current_chunk.append([current_pos])\n",
        "    elif candidate == 'I-' + current_tag:\n",
        "        if prediction and (current_pos == 0 or current_pos > 0 and prev.split('-', 1)[-1] != current_tag):\n",
        "            current_chunk.append([current_pos])\n",
        "        if not prediction and (current_pos == 0 or current_pos > 0 and prev == 'O'):\n",
        "            current_chunk.append([current_pos])\n",
        "    elif current_pos > 0 and prev.split('-', 1)[-1] == current_tag:\n",
        "        if len(current_chunk) > 0:\n",
        "            current_chunk[-1].append(current_pos - 1)\n",
        "\n",
        "def _update_last_chunk(current_chunk, current_pos):\n",
        "    if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
        "        current_chunk[-1].append(current_pos - 1)\n",
        "\n",
        "def _tag_precision_recall_f1(tp, fp, fn):\n",
        "    precision, recall, f1 = 0, 0, 0\n",
        "    if tp + fp > 0:\n",
        "        precision = tp / (tp + fp) * 100\n",
        "    if tp + fn > 0:\n",
        "        recall = tp / (tp + fn) * 100\n",
        "    if precision + recall > 0:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "def _aggregate_metrics(results, total_correct):\n",
        "    total_true_entities = 0\n",
        "    total_predicted_entities = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    for tag, tag_metrics in results.items():\n",
        "        n_pred = tag_metrics['n_predicted_entities']\n",
        "        n_true = tag_metrics['n_true_entities']\n",
        "        total_true_entities += n_true\n",
        "        total_predicted_entities += n_pred\n",
        "        total_precision += tag_metrics['precision'] * n_pred\n",
        "        total_recall += tag_metrics['recall'] * n_true\n",
        "    \n",
        "    accuracy = 0\n",
        "    if total_true_entities > 0:\n",
        "        accuracy = total_correct / total_true_entities * 100\n",
        "    else:\n",
        "        print('CAUTION! Accuracy equals zero because there are no '\\\n",
        "              'correct entities. Check the correctness of your data.')\n",
        "    if total_predicted_entities > 0:\n",
        "        total_precision = total_precision / total_predicted_entities\n",
        "    total_recall = total_recall / total_true_entities\n",
        "    if total_precision + total_recall > 0:\n",
        "        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall)\n",
        "    return total_true_entities, total_predicted_entities, \\\n",
        "           total_precision, total_recall, total_f1, accuracy\n",
        "\n",
        "def _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct):\n",
        "    print('processed {len} tokens ' \\\n",
        "          'with {tot_true} phrases; ' \\\n",
        "          'found: {tot_pred} phrases; ' \\\n",
        "          'correct: {tot_cor}.\\n'.format(len=n_tokens,\n",
        "                                         tot_true=total_true_entities,\n",
        "                                         tot_pred=total_predicted_entities,\n",
        "                                         tot_cor=total_correct))\n",
        "\n",
        "def _print_metrics(accuracy, total_precision, total_recall, total_f1):\n",
        "    print('precision:  {tot_prec:.2f}%; ' \\\n",
        "          'recall:  {tot_recall:.2f}%; ' \\\n",
        "          'F1:  {tot_f1:.2f}\\n'.format(acc=accuracy,\n",
        "                                           tot_prec=total_precision,\n",
        "                                           tot_recall=total_recall,\n",
        "                                           tot_f1=total_f1))\n",
        "\n",
        "def _print_tag_metrics(tag, tag_results):\n",
        "    print(('\\t%12s' % tag) + ': precision:  {tot_prec:6.2f}%; ' \\\n",
        "                               'recall:  {tot_recall:6.2f}%; ' \\\n",
        "                               'F1:  {tot_f1:6.2f}; ' \\\n",
        "                               'predicted:  {tot_predicted:4d}\\n'.format(tot_prec=tag_results['precision'],\n",
        "                                                                         tot_recall=tag_results['recall'],\n",
        "                                                                         tot_f1=tag_results['f1'],\n",
        "                                                                         tot_predicted=tag_results['n_predicted_entities']))\n",
        "\n",
        "def precision_recall_f1(y_true, y_pred, print_results=True, short_report=False):\n",
        "    # Find all tags\n",
        "    tags = sorted(set(tag[2:] for tag in y_true + y_pred if tag != 'O'))\n",
        "\n",
        "    results = OrderedDict((tag, OrderedDict()) for tag in tags)\n",
        "    n_tokens = len(y_true)\n",
        "    total_correct = 0\n",
        "\n",
        "    # For eval_conll_try we find all chunks in the ground truth and prediction\n",
        "    # For each chunk we store starting and ending indices\n",
        "    for tag in tags:\n",
        "        true_chunk = list()\n",
        "        predicted_chunk = list()\n",
        "        for position in range(n_tokens):\n",
        "            _update_chunk(y_true[position], y_true[position - 1], tag, true_chunk, position)\n",
        "            _update_chunk(y_pred[position], y_pred[position - 1], tag, predicted_chunk, position, True)\n",
        "\n",
        "        _update_last_chunk(true_chunk, position)\n",
        "        _update_last_chunk(predicted_chunk, position)\n",
        "\n",
        "        # Then we find all correctly classified intervals\n",
        "        # True positive results\n",
        "        tp = sum(chunk in predicted_chunk for chunk in true_chunk)\n",
        "        total_correct += tp\n",
        "\n",
        "        # And then just calculate errors of the first and second kind\n",
        "        # False negative\n",
        "        fn = len(true_chunk) - tp\n",
        "        # False positive\n",
        "        fp = len(predicted_chunk) - tp\n",
        "        precision, recall, f1 = _tag_precision_recall_f1(tp, fp, fn)\n",
        "\n",
        "        results[tag]['precision'] = precision\n",
        "        results[tag]['recall'] = recall\n",
        "        results[tag]['f1'] = f1\n",
        "        results[tag]['n_predicted_entities'] = len(predicted_chunk)\n",
        "        results[tag]['n_true_entities'] = len(true_chunk)\n",
        "\n",
        "    total_true_entities, total_predicted_entities, \\\n",
        "           total_precision, total_recall, total_f1, accuracy = _aggregate_metrics(results, total_correct)\n",
        "\n",
        "    if print_results:\n",
        "      print(\"Entrei\")\n",
        "      _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct)\n",
        "      _print_metrics(accuracy, total_precision, total_recall, total_f1)\n",
        "\n",
        "      if not short_report:\n",
        "          for tag, tag_results in results.items():\n",
        "              _print_tag_metrics(tag, tag_results)\n",
        "    return results"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnowiQmr7Scn"
      },
      "source": [
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag\n",
        "        # to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJT3VicC4JWN",
        "outputId": "ce927489-98c6-4cc5-b4d5-26a352c25a89"
      },
      "source": [
        "\n",
        "\n",
        "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200,PAD_index=token2idx['<PAD>'])\n",
        "\n",
        "batch_size = 32\n",
        "n_epochs = 4\n",
        "learning_rate = .005\n",
        "learning_rate_decay = 2**(.5)\n",
        "dropout_keep_probability = .5"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-14-0b130ffaacc1>:35: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:750: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:699: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-glVozlx4-FE",
        "outputId": "593d6d0a-8a78-489c-efb1-2399214235b5"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "    # For each epoch evaluate the model on train and validation data\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training... \n",
            "\n",
            "-------------------- Epoch 1 of 4 --------------------\n",
            "Train data evaluation:\n",
            "Entrei\n",
            "processed 48855 tokens with 1496 phrases; found: 32288 phrases; correct: 52.\n",
            "\n",
            "precision:  0.16%; recall:  3.48%; F1:  0.31\n",
            "\n",
            "-------------------- Epoch 2 of 4 --------------------\n",
            "Train data evaluation:\n",
            "Entrei\n",
            "processed 48855 tokens with 1496 phrases; found: 3 phrases; correct: 0.\n",
            "\n",
            "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
            "\n",
            "-------------------- Epoch 3 of 4 --------------------\n",
            "Train data evaluation:\n",
            "Entrei\n",
            "processed 48855 tokens with 1496 phrases; found: 0 phrases; correct: 0.\n",
            "\n",
            "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
            "\n",
            "-------------------- Epoch 4 of 4 --------------------\n",
            "Train data evaluation:\n",
            "Entrei\n",
            "processed 48855 tokens with 1496 phrases; found: 12 phrases; correct: 0.\n",
            "\n",
            "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
            "\n",
            "...training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDCJyi506lly",
        "outputId": "a6a9f9ac-8475-4eba-9e67-072164598215"
      },
      "source": [
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "Entrei\n",
            "processed 48855 tokens with 1496 phrases; found: 186 phrases; correct: 7.\n",
            "\n",
            "precision:  3.76%; recall:  0.47%; F1:  0.83\n",
            "\n",
            "\t     company: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t    facility: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t     geo-loc: precision:   25.00%; recall:    0.36%; F1:    0.71; predicted:     4\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t       other: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
            "\n",
            "\t      person: precision:    3.33%; recall:    1.34%; F1:    1.91; predicted:   180\n",
            "\n",
            "\t     product: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtzZ-HVuBezp",
        "outputId": "d4704d41-e393-4784-aa9a-4f03c245ed5c"
      },
      "source": [
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Test set quality: --------------------\n",
            "Entrei\n",
            "processed 65745 tokens with 3473 phrases; found: 97 phrases; correct: 2.\n",
            "\n",
            "precision:  2.06%; recall:  0.06%; F1:  0.11\n",
            "\n",
            "\t     company: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t    facility: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t     geo-loc: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     3\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t       other: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t      person: precision:    2.13%; recall:    0.41%; F1:    0.69; predicted:    94\n",
            "\n",
            "\t     product: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XnEvhA1Bi0V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}