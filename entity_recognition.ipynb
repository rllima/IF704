{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entity_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxhTOA5K74dYvc+DkRUWZ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rllima/IF704/blob/main/entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHo8FQhOIYlx",
        "outputId": "ff19350e-d6b0-4a1e-d000-dce7d84ee5e1"
      },
      "source": [
        "#http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdgpa8T5V_Do",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ed43c9-945f-4f38-f8ea-bd84e4cd4568"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rllima/IF704/main/data/train.csv\n",
        "!wget https://raw.githubusercontent.com/rllima/IF704/main/data/test.csv"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-18 00:45:57--  https://raw.githubusercontent.com/rllima/IF704/main/data/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 354161 (346K) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>] 345.86K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-08-18 00:45:57 (8.46 MB/s) - ‘train.csv’ saved [354161/354161]\n",
            "\n",
            "--2021-08-18 00:45:58--  https://raw.githubusercontent.com/rllima/IF704/main/data/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 568765 (555K) [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 555.43K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-08-18 00:45:58 (10.5 MB/s) - ‘test.csv’ saved [568765/568765]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ARiDXiWv3X"
      },
      "source": [
        "from collections import defaultdict\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njvOebSiqlP2"
      },
      "source": [
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            # Replace all urls with <URL> token\n",
        "            # Replace all users with <USR> token\n",
        "            if token.find('http://') == 0 or token.find('https://') == 0:\n",
        "                token = '<URL>'\n",
        "            if token[0] == '@':\n",
        "                token = '<USR>'\n",
        "            \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO--dNtIwfjo"
      },
      "source": [
        "Prepare dictionaries\n",
        "\n",
        "To train a neural network, we will use two mappings:\n",
        "\n",
        "{token} →\n",
        "→\n",
        " {token id}: address the row in embeddings matrix for the current token;\n",
        "{tag} →\n",
        "→\n",
        " {tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
        "Now you need to implement the function build_dict which will return {token or tag} →\n",
        "→\n",
        " {index} and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OetWT43dvP0V"
      },
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
        "    # Add special tokens (or tags) to the dictionaries.\n",
        "    # The first special token must have index 0.\n",
        "    \n",
        "    # Mapping tok2idx should contain each token or tag only once. \n",
        "    # To do so, you should extract unique tokens/tags from the tokens_or_tags variable\n",
        "    # and then index them (for example, you can add them into the list idx2tok\n",
        "    # and for each token/tag save the index into tok2idx).\n",
        "\n",
        "    for twt in tokens_or_tags:\n",
        "        for tok in twt:\n",
        "            idx2tok.append(tok)\n",
        "    idx2tok = list(set(idx2tok))\n",
        "    idx2tok = special_tokens + idx2tok\n",
        "    for i, v in enumerate(idx2tok):\n",
        "        tok2idx[v] = i\n",
        "    \n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUw3A1jff7Xj"
      },
      "source": [
        "train_tokens, train_tags = read_data('train.csv')\n",
        "test_tokens, test_tags = read_data('test.csv')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NTaq2SEWzJm"
      },
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTCcHeTbgi5S",
        "outputId": "47d4c068-3b20-4d00-ccff-d3d5dfda120e"
      },
      "source": [
        "len(set(idx2token)) == len(idx2token)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX4OVB8Li2Is"
      },
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8RS6JjgxA-f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}